[
  {
    "filename": "app.py",
    "skills": {
      "Hybrid (Regex+NLP)": [
        "deep learning",
        "api",
        "ml modeling",
        "encoding",
        "front end",
        "web development",
        "data engineering",
        "data loading",
        "statistics",
        "visualization",
        "etl",
        "data cleaning",
        "database",
        "nlp",
        "embedding"
      ]
    },
    "content": "import streamlit as st\nimport os\nimport json\nfrom collections import Counter\nimport pandas as pd\nfrom ollama_rag import query_ollama\nimport subprocess\nfrom tools.ML.llama_skill_identifier import query_codellama_ollama, extract_skills_from_llama_response\nfrom tools.web_search import search_duckduckgo\nimport time\nfrom duckduckgo_search.exceptions import DuckDuckGoSearchException\nimport chromadb\nfrom chromadb.config import Settings\nimport sys\nimport tempfile\nimport importlib.util\nimport spacy\nimport random\nfrom ingest_data import list_supported_files\nfrom utils import load_and_split_documents, check_ollama_availability, EMBEDDING_MODEL, LLM_MODEL, parallel_embed_chunks\n        import subprocess\n        import sys\n    import requests\n                    from tools.ml_nlp_skill_identifier import identify_skills_nlp\n            import shutil\n                    import re\n                    import_lines = re.findall(r'^(?:from|import)\\s+([\\w\\.]+)', code, re.MULTILINE)\n            import pandas as pd\n                                from tools.ml_nlp_skill_identifier import identify_skills_nlp\n                                from ollama_rag import query_ollama\n    from tools.render_quarto_template import render_quarto_template\n        import threading\n            import tempfile\nsys.path.insert(0, os.path.join(os.path.dirname(__file__), 'ingest'))\nsys.path.insert(0, os.path.abspath(os.path.dirname(__file__)))\n\n# Ensure set_page_config is the first Streamlit command\nst.set_page_config(page_title=\"Skills Gap Data Explorer\", layout=\"wide\")\n\n# --- Ensure skillset_cache is always initialized ---\nif \"skillset_cache\" not in st.session_state:\n    st.session_state[\"skillset_cache\"] = {}\nskillset_cache = st.session_state[\"skillset_cache\"]\n\nBASE_DIR = os.path.abspath(os.path.dirname(__file__))\nINGESTED_PATH = os.path.join(BASE_DIR, 'downstream', 'ingested_files.json')\nCATEGORIZED_PATH = os.path.join(BASE_DIR, 'downstream', 'skills_categorized.json')\nWEBREF_PATH = os.path.join(BASE_DIR, 'downstream', 'web_references.json')\nWEBREF_FILTERED_PATH = os.path.join(BASE_DIR, 'downstream', 'web_references_filtered.json')\nDOC_DIR = os.path.join(BASE_DIR, 'downstream', 'doc')\n# Ensure DOC_DIR exists\nos.makedirs(DOC_DIR, exist_ok=True)\nMD_REPORT = os.path.join(DOC_DIR, 'skills_gap_report.md')\nQMD_REPORT = os.path.join(DOC_DIR, 'skills_gap_report.qmd')\nPDF_REPORT = os.path.join(DOC_DIR, 'skills_gap_report.pdf')\nHTML_REPORT = os.path.join(DOC_DIR, 'skills_gap_report.html')\nTOOLS_DIR = os.path.join(BASE_DIR, 'tools')\n\n# --- ChromaDB and Embedding Setup ---\nCHROMA_DIR = os.path.join(os.path.dirname(__file__), 'chromadb_store')\nCOLLECTION_NAME = 'my_documents'\nclient = chromadb.Client(Settings(persist_directory=CHROMA_DIR))\ncollection = client.get_or_create_collection(COLLECTION_NAME)\n\n# Add chunk size option to sidebar\nchunk_size = st.sidebar.number_input(\"Chunk size (characters)\", min_value=200, max_value=5000, value=1000, step=100, help=\"Set the chunk size for splitting documents.\")\n\n# Check if spaCy en_core_web_sm is installed, if not, download it with a spinner\nwith st.spinner(\"Checking spaCy model (en_core_web_sm)...\"):\n    try:\n        spacy.load(\"en_core_web_sm\")\n        spacy_model_ready = True\n    except OSError:\n        st.info(\"Downloading spaCy model: en_core_web_sm (first time only)...\")\n        result = subprocess.run([sys.executable, \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"], capture_output=True, text=True)\n        if result.returncode == 0:\n            st.success(\"spaCy model en_core_web_sm downloaded!\")\n            spacy_model_ready = True\n        else:\n            st.error(\"Failed to download spaCy model en_core_web_sm. Some features may not work.\")\n            spacy_model_ready = False\n\n# Replace embed_chunks_with_ollama to use fallback if Ollama is not running\ndef is_ollama_running():\n    try:\n        r = requests.get(\"http://localhost:11434/api/tags\", timeout=2)\n        return r.status_code == 200\n    except Exception:\n        return False\n\nollama_available = is_ollama_running()\n\ndef embed_chunks_with_ollama(chunks):\n    progress = st.progress(0, text=\"Embedding chunks...\")\n    fallback_used = False\n    def progress_callback(done, total):\n        progress.progress(done/total, text=f\"Embedding chunk {done}/{total}\")\n    def embed_func(chunk):\n        if ollama_available:\n            try:\n                embedding = query_ollama(chunk['content'], model=EMBEDDING_MODEL)\n                if not isinstance(embedding, list):\n                    print(f\"[DEBUG] Ollama returned non-list: {embedding} (type: {type(embedding)})\")\n                    fallback_used = True\n                    embedding = [random.uniform(-1, 1) for _ in range(384)]\n            except Exception as e:\n                print(f\"Embedding failed: {e}\")\n                fallback_used = True\n                embedding = [random.uniform(-1, 1) for _ in range(384)]\n        else:\n            fallback_used = True\n            embedding = [random.uniform(-1, 1) for _ in range(384)]\n        chunk['embedding'] = embedding\n        return chunk\n    results = parallel_embed_chunks(chunks, embed_func, max_workers=4, timeout=120, progress_callback=progress_callback)\n    progress.empty()\n    if fallback_used:\n        st.warning(\"Ollama did not return valid embeddings for some or all chunks. Fallback embeddings were used. Check logs for details.\")\n    return [r for r in results if r is not None]\n\ndef add_documents_to_chromadb(documents, source_name):\n    try:\n        ids = [f\"{source_name}_{i}\" for i in range(len(documents))]\n        texts = [doc['content'] for doc in documents]\n        embeddings = [doc['embedding'] for doc in documents]\n        metadatas = [doc.get('metadata', {'source': source_name}) for doc in documents]\n        collection.add(\n            ids=ids,\n            documents=texts,\n            embeddings=embeddings,\n            metadatas=metadatas\n        )\n        return True, len(documents)\n    except Exception as e:\n        print(f\"Error adding to ChromaDB: {e}\")\n        return False, 0\n\ndef list_loaded_documents_chroma():\n    try:\n        all_metas = collection.get(include=['metadatas'])['metadatas']\n        sources = set()\n        for meta in all_metas:\n            if meta and 'source' in meta:\n                sources.add(meta['source'])\n        return list(sources)\n    except Exception as e:\n        print(f\"Error listing loaded documents: {e}\")\n        return []\n\n# --- Sidebar for Uploading and Batch Ingest ---\nst.sidebar.header(\"Add Content Source (ChromaDB)\")\n\nuploaded_files = st.sidebar.file_uploader(\n    \"Upload one or more documents\",\n    type=[\"pdf\", \"txt\", \"csv\", \"xlsx\", \"xls\", \"json\", \"db\", \"sqlite\", \"sqlite3\", \"sql\", \"py\"],\n    help=\"Supports PDF, TXT, CSV, Excel, JSON, SQLite, SQL, and Python files.\",\n    accept_multiple_files=True\n)\n\nif uploaded_files is not None:\n    st.sidebar.success(f\"Uploader ready. {len(uploaded_files)} file(s) selected.\")\nelse:\n    st.sidebar.info(\"Uploader ready. No files selected yet.\")\n\nurl_input = st.sidebar.text_input(\n    \"Or enter a Web URL\",\n    placeholder=\"https://example.com\"\n)\n\ndb_uri_input = st.sidebar.text_input(\n    \"Or enter PostgreSQL DB URI (schema only)\",\n    placeholder=\"postgresql://user:pass@host:port/dbname\"\n)\n\nif st.sidebar.button(\"Process Content Source (ChromaDB)\"):\n    temp_dirs = []\n    content_sources = []\n    if uploaded_files:\n        for uploaded_file in uploaded_files:\n            temp_dir = tempfile.mkdtemp()\n            temp_dirs.append(temp_dir)\n            file_path = os.path.join(temp_dir, uploaded_file.name)\n            with open(file_path, \"wb\") as f:\n                f.write(uploaded_file.getbuffer())\n            content_sources.append({\"type\": \"file\", \"path\": file_path, \"name\": uploaded_file.name})\n            st.sidebar.info(f\"Processing file: {uploaded_file.name}\")\n    if url_input:\n        content_sources.append({\"type\": \"url\", \"url\": url_input})\n        st.sidebar.info(f\"Processing URL: {url_input}\")\n    if db_uri_input:\n        content_sources.append({\"type\": \"db\", \"uri\": db_uri_input})\n        st.sidebar.info(f\"Processing DB URI: {db_uri_input.split('@')[0]}...\")\n    if not content_sources:\n        st.sidebar.warning(\"Please provide at least one source: upload files, enter a URL, or enter a DB URI.\")\n    for content_source_info in content_sources:\n        source_type = content_source_info[\"type\"]\n        source_identifier = content_source_info.get(\"name\") or content_source_info.get(\"url\") or content_source_info.get(\"uri\")\n        with st.spinner(f\"Loading and splitting content from {source_type}: {source_identifier}... This might take a moment.\"):\n            documents, source_name = load_and_split_documents(\n                file_path=content_source_info.get(\"path\"),\n                url=content_source_info.get(\"url\"),\n                db_uri=content_source_info.get(\"uri\"),\n                chunk_size=chunk_size\n            )\n        if documents:\n            embedded_docs = embed_chunks_with_ollama(documents)\n            display_name = source_name if source_name != \"Unknown\" else source_identifier\n            with st.spinner(f\"Adding {len(embedded_docs)} chunk(s) from {display_name} to ChromaDB...\"):\n                success, count = add_documents_to_chromadb(embedded_docs, display_name)\n                if success:\n                    st.sidebar.success(f\"Successfully added {count} chunk(s) from {display_name} to ChromaDB!\")\n                    # --- Automatic skill extraction ---\n                    for doc in embedded_docs:\n                        code = doc['content']\n                        skills = identify_skills_nlp(code)\n                        skillset_cache.setdefault(display_name, {\"Hybrid (Regex+NLP)\": set()})\n                        skillset_cache[display_name][\"Hybrid (Regex+NLP)\"] |= skills\n                    # --- Save code content for this file ---\n                    st.session_state.setdefault('file_code_content', {})\n                    # Concatenate all chunks for this file\n                    file_code = ''.join([doc['content'] for doc in embedded_docs if doc.get('content')])\n                    st.session_state['file_code_content'][display_name] = file_code\n                else:\n                    st.sidebar.error(f\"Failed to add content from {display_name}. Check console.\")\n        else:\n            st.sidebar.error(f\"Could not load or split content from {source_identifier}. Check console.\")\n    # Save extracted skills to JSON\n    categorized_skills_list = []\n    for fname, model_skills in skillset_cache.items():\n        # Add code content if available\n        code_content = st.session_state.get('file_code_content', {}).get(fname, '')\n        categorized_skills_list.append({\n            \"filename\": fname,\n            \"skills\": {k: list(v) for k, v in model_skills.items()},\n            \"content\": code_content\n        })\n    with open(os.path.join(BASE_DIR, 'downstream', 'skills_categorized.json'), 'w', encoding='utf-8') as f:\n        json.dump(categorized_skills_list, f, indent=2)\n    st.sidebar.success(\"Skills extracted and saved to downstream/skills_categorized.json!\")\n    for temp_dir in temp_dirs:\n        if os.path.exists(temp_dir):\n            shutil.rmtree(temp_dir)\n\nif st.sidebar.button(\"Batch Ingest from 'ingest' Folder (ChromaDB)\"):\n    ingest_folder = os.path.join(os.path.dirname(__file__), 'ingest')\n    supported_files = list_supported_files(ingest_folder, ['.py', '.csv', '.txt', '.pdf', '.json', '.xlsx', '.xls'])\n    for filename in supported_files:\n        file_path = os.path.join(ingest_folder, filename)\n        with st.spinner(f\"Batch loading and splitting: {filename}\"):\n            documents, source_name = load_and_split_documents(file_path=file_path, chunk_size=chunk_size)\n        if documents:\n            embedded_docs = embed_chunks_with_ollama(documents)\n            with st.spinner(f\"Adding {len(embedded_docs)} chunk(s) from {filename} to ChromaDB...\"):\n                success, count = add_documents_to_chromadb(embedded_docs, filename)\n                if success:\n                    st.sidebar.success(f\"Batch: Added {count} chunk(s) from {filename} to ChromaDB!\")\n                else:\n                    st.sidebar.error(f\"Batch: Failed to add content from {filename}. Check console.\")\n        else:\n            st.sidebar.error(f\"Batch: Could not load or split content from {filename}. Check console.\")\n\n# Show loaded ChromaDB sources\nst.sidebar.markdown(\"--- \")\nst.sidebar.subheader(\"Loaded Documents (ChromaDB)\")\nloaded_sources_chroma = list_loaded_documents_chroma()\nif loaded_sources_chroma:\n    for i, source in enumerate(loaded_sources_chroma):\n        st.sidebar.text(f\"{i+1}. {source}\")\nelse:\n    st.sidebar.text(\"No documents loaded yet in ChromaDB\")\n    st.sidebar.info(\"\ud83d\udc46 Add content by uploading a file, entering a URL, or using batch ingest.\")\nst.sidebar.markdown(\"--- \")\nst.sidebar.caption(f\"Vector data is managed by ChromaDB\")\n\n# --- Web search function using cached skills ---\ndef run_skill_driven_web_search(skills_by_file):\n    results = []\n    for file, model_skills in skills_by_file.items():\n        for model, skills in model_skills.items():\n            for skill in skills:\n                try:\n                    search_results = search_duckduckgo(f\"learn {skill}\", max_results=3)\n                    for r in search_results:\n                        results.append({\n                            'file': file,\n                            'model': model,\n                            'skill': skill,\n                            'title': r.get('title', ''),\n                            'url': r.get('url', ''),\n                            'snippet': r.get('snippet', '')\n                        })\n                    time.sleep(1)  # Add a delay to avoid rate limiting\n                except DuckDuckGoSearchException as e:\n                    results.append({\n                        'file': file,\n                        'model': model,\n                        'skill': skill,\n                        'title': 'Rate limit hit or search failed',\n                        'url': '',\n                        'snippet': str(e)\n                    })\n    return results\n\n# --- Stub: Pass cached skill sets to webscraping pipeline for targeted search ---\n# def run_webscraping_with_skills(skillset_cache):\n#     pass  # To be implemented in a future sprint \n\n# Global cache for skill sets by file and model (now in session state)\nif \"skill_web_refs\" not in st.session_state:\n    st.session_state[\"skill_web_refs\"] = {}\n\nst.title(\"Skills Gap Data Explorer\")\n\n# Sidebar navigation\nview_option = st.sidebar.radio(\n    \"Select View:\",\n    (\"Analytics\", \"Ingested Files\", \"Skill Categorization\", \"RAG Q&A\", \"Web References\", \"Generate Report\")\n)\n\ndef load_json(path):\n    if os.path.exists(path):\n        with open(path, 'r', encoding='utf-8') as f:\n            return json.load(f)\n    return []\n\ningested_files = load_json(INGESTED_PATH)\ncategorized_skills = load_json(CATEGORIZED_PATH)\nweb_references = load_json(WEBREF_PATH)\nweb_references_filtered = load_json(WEBREF_FILTERED_PATH)\n\ndef file_download_link(filepath, label):\n    if os.path.exists(filepath):\n        # st.download_button(label, f, file_name=os.path.basename(filepath))\n        st.info(f\"{label}: {filepath}\")\n\ndef file_url_info(filepath, label):\n    if os.path.exists(filepath):\n        file_url = f\"file://{filepath}\"\n        st.info(f\"[{label}]({file_url}) (opens in a new tab)\", icon=\"\ud83c\udf10\")\n\nif view_option == \"Analytics\":\n    # --- Live Analytics from ChromaDB ---\n    file_sources = list_loaded_documents_chroma()\n    all_docs = collection.get(include=['documents', 'metadatas'])\n\n    # Number of files ingested\n    num_files = len(file_sources)\n    st.metric(\"Total Files Ingested\", num_files)\n\n    # File types breakdown (from metadata or file extension)\n    file_types = [os.path.splitext(src)[1] for src in file_sources]\n    if file_types:\n        type_counts = pd.Series(file_types).value_counts()\n        st.subheader(\"File Types Breakdown (Live)\")\n        type_df = pd.DataFrame(type_counts).reset_index()\n        type_df.columns = [\"File Type\", \"Count\"]\n        st.dataframe(type_df, use_container_width=True)\n        st.bar_chart(type_df.set_index(\"File Type\"))\n\n    # Chunk counts per file\n    chunk_counts = {}\n    for meta in all_docs['metadatas']:\n        if meta and 'source' in meta:\n            chunk_counts[meta['source']] = chunk_counts.get(meta['source'], 0) + 1\n    if chunk_counts:\n        st.subheader(\"Chunk Counts per File (Live)\")\n        chunk_df = pd.DataFrame(list(chunk_counts.items()), columns=[\"Filename\", \"Chunks\"])\n        st.dataframe(chunk_df, use_container_width=True)\n        st.bar_chart(chunk_df.set_index(\"Filename\"))\n\n    # (Optional) You can keep or comment out the old static analytics code below\n\nelif view_option == \"Ingested Files\":\n    st.header(\"Ingested Files\")\n    if not ingested_files:\n        st.info(\"No ingested files found.\")\n    else:\n        filenames = [f[\"filename\"] for f in ingested_files]\n        selected_file = st.selectbox(\"Select a file to view its content:\", filenames)\n        file_data = next((f for f in ingested_files if f[\"filename\"] == selected_file), None)\n        if file_data:\n            st.subheader(f\"File: {file_data['filename']} ({file_data.get('filetype', 'unknown')})\")\n            if file_data['filetype'] == '.csv' and isinstance(file_data['content'], list):\n                st.write(\"Preview (first 10 rows):\")\n                st.table(file_data['content'][:10])\n            else:\n                st.code(file_data['content'], language=file_data.get('filetype', 'unknown').replace('.', ''))\n\nelif view_option == \"Skill Categorization\":\n    st.header(\"Skill Categorization Results\")\n    # --- Debug output ---\n    st.write(\"DEBUG: categorized_skills =\", categorized_skills)\n    st.write(\"DEBUG: skillset_cache =\", skillset_cache)\n    if not categorized_skills:\n        st.info(\"No skill categorization results found.\")\n    else:\n        # --- New: Table view for file, skills, and libraries ---\n        table_rows = []\n        for entry in categorized_skills:\n            filename = entry.get('filename', 'unknown')\n            skills = []\n            libraries = set()\n            # Try to get all skills from all models\n            if 'skills' in entry:\n                # If skills is a dict of model->skills\n                if isinstance(entry['skills'], dict):\n                    for model, skill_list in entry['skills'].items():\n                        if isinstance(skill_list, list):\n                            skills.extend(skill_list)\n                elif isinstance(entry['skills'], list):\n                    skills.extend(entry['skills'])\n            # --- Extract libraries from content for .py files ---\n            if filename.endswith('.py'):\n                code = entry.get('content', '')\n                if code:\n                    # Find all import and from ... import ... statements\n                    for lib in import_lines:\n                        # Only take the top-level package\n                        libraries.add(lib.split('.')[0])\n            table_rows.append({\n                'file': filename,\n                'skills': ', '.join(sorted(set(skills))),\n                'libraries': ', '.join(sorted(libraries)) if libraries else ''\n            })\n        if table_rows:\n            df = pd.DataFrame(table_rows)\n            st.dataframe(df, use_container_width=True)\n        filenames = [f[\"filename\"] for f in categorized_skills]\n        model_options = [\"Hybrid (Regex+NLP)\", \"phi4:mini (Ollama)\", \"Code Llama (Ollama)\"]\n        selected_models = st.multiselect(\"Select model(s) for skill extraction:\", model_options, default=model_options)\n        tab1, tab2 = st.tabs([\"Detail View\", \"Tabular View\"])\n        with tab1:\n            selected_files = st.multiselect(\"Select one or more files to view their skills:\", filenames, default=filenames)\n            if st.button(\"Extract and Cache Skills\"):\n                with st.spinner(\"Extracting and caching skills for selected files and models...\"):\n                    for selected_file in selected_files:\n                        skill_data = next((f for f in categorized_skills if f[\"filename\"] == selected_file), None)\n                        if skill_data:\n                            code = skill_data.get('content', '')\n                            skillset_cache.setdefault(selected_file, {})\n                            if \"Hybrid (Regex+NLP)\" in selected_models:\n                                hybrid_skills = identify_skills_nlp(code)\n                                skillset_cache[selected_file][\"Hybrid (Regex+NLP)\"] = hybrid_skills\n                            if \"phi4:mini (Ollama)\" in selected_models:\n                                phi4_prompt = f\"Given the following code, list the main programming skills, libraries, and topics it demonstrates. Return the answer as a comma-separated list of skills/topics only (no explanation):\\n\\nCODE:\\n{code}\"\n                                phi4_response = query_ollama(phi4_prompt, model='phi4-mini:latest')\n                                phi4_skills = [s.strip() for s in phi4_response.split(',') if s.strip()]\n                                skillset_cache[selected_file][\"phi4:mini (Ollama)\"] = phi4_skills\n                            if \"Code Llama (Ollama)\" in selected_models:\n                                llama_response = query_codellama_ollama(code)\n                                llama_skills = extract_skills_from_llama_response(llama_response)\n                                skillset_cache[selected_file][\"Code Llama (Ollama)\"] = llama_skills\n                st.success(\"Skill extraction and caching complete! You can now use the web search functionality.\")\n            # Display cached skills for selected files/models\n            for selected_file in selected_files:\n                skill_data = next((f for f in categorized_skills if f[\"filename\"] == selected_file), None)\n                if skill_data:\n                    st.subheader(f\"File: {skill_data['filename']} ({skill_data.get('filetype', 'unknown')})\")\n                    for model in selected_models:\n                        if model in skillset_cache.get(selected_file, {}):\n                            st.markdown(f\"**{model} Skills:**\")\n                            st.write(skillset_cache[selected_file][model])\n        with tab2:\n            selected_files = st.multiselect(\"Select files for table view:\", filenames, default=filenames, key=\"table_view_select\")\n            # Prepare data for each section\n            functions_data = {f: next((x['skills'].get('functions', []) for x in categorized_skills if x['filename'] == f), []) for f in selected_files}\n            libraries_data = {f: next((x['skills'].get('libraries', []) for x in categorized_skills if x['filename'] == f), []) for f in selected_files}\n            comments_data = {f: next((x['skills'].get('comments', []) for x in categorized_skills if x['filename'] == f), []) for f in selected_files}\n            st.markdown(\"### Functions Table\")\n            st.dataframe({fn: functions_data[fn] for fn in selected_files})\n            st.markdown(\"### Libraries Table\")\n            st.dataframe({fn: libraries_data[fn] for fn in selected_files})\n            st.markdown(\"### Comments Table\")\n            st.dataframe({fn: comments_data[fn] for fn in selected_files})\n            # --- New: File Name and Library Table ---\n            library_rows = []\n            for f in selected_files:\n                for lib in libraries_data[f]:\n                    library_rows.append({\"Filename\": f, \"Library\": lib})\n            if library_rows:\n                st.markdown(\"### File Name and Libraries Used (One per Row)\")\n                st.dataframe(pd.DataFrame(library_rows))\n\nelif view_option == \"RAG Q&A\":\n    st.header(\"RAG Q&A (Ollama)\")\n    st.write(\"Ask questions or request summaries using the local Ollama model (phi4-mini:latest). Optionally, select a file to provide context.\")\n    context = \"\"\n    if categorized_skills:\n        filenames = [f[\"filename\"] for f in categorized_skills]\n        selected_file = st.selectbox(\"Select a file for context (optional):\", [\"None\"] + filenames)\n        if selected_file != \"None\":\n            skill_data = next((f for f in categorized_skills if f[\"filename\"] == selected_file), None)\n            if skill_data:\n                skills = skill_data['skills']\n                context = f\"\\n\\nContext from {skill_data['filename']} ({skill_data.get('filetype', 'unknown')}):\\n\"\n                context += f\"Functions: {', '.join(skills.get('functions', [])) or 'None'}\\n\"\n                context += f\"Libraries: {', '.join(skills.get('libraries', [])) or 'None'}\\n\"\n                context += f\"Comments: {' | '.join(skills.get('comments', [])) or 'None'}\\n\"\n    user_prompt = st.text_area(\"Enter your question or prompt:\")\n    if st.button(\"Ask Ollama\") and user_prompt.strip():\n        with st.spinner(\"Querying Ollama (phi4-mini:latest)...\"):\n            full_prompt = user_prompt + context\n            response = query_ollama(full_prompt, model='phi4-mini:latest')\n        st.markdown(\"**Ollama Response:**\")\n        st.write(response)\n\nelif view_option == \"Web References\":\n    st.header(\"Web References: Gold Standard & Self-Learning Platforms\")\n    ref_type = st.radio(\"Show:\", [\"All References\", \"Filtered (High-Quality) References\", \"Skill-Driven Web References\"])\n    refs = web_references if ref_type == \"All References\" else web_references_filtered\n    # Skill-driven web search integration\n    if ref_type == \"Skill-Driven Web References\":\n        st.markdown(\"**Run a targeted web search using cached skills from skill categorization.**\")\n        # Let user select files to use for skill-driven search\n        if not skillset_cache:\n            st.info(\"No cached skills found. Run skill categorization first.\")\n        else:\n            files_with_skills = list(skillset_cache.keys())\n            selected_files = st.multiselect(\"Select files to use for skill-driven web search:\", files_with_skills, default=files_with_skills)\n            cache_key = tuple(sorted(selected_files))\n            if st.button(\"Run Skill-Driven Web Search\"):\n                selected_skills = {f: skillset_cache[f] for f in selected_files}\n                skill_refs = run_skill_driven_web_search(selected_skills)\n                st.session_state[\"skill_web_refs\"][cache_key] = skill_refs\n            # Use cached results if available\n            skill_refs = st.session_state[\"skill_web_refs\"].get(cache_key, [])\n            if skill_refs:\n                st.success(f\"Found {len(skill_refs)} skill-driven web references.\")\n                skill_ref_df = pd.DataFrame(skill_refs)\n                skill_ref_df['url'] = skill_ref_df['url'].apply(lambda x: f\"[link]({x})\" if x else \"\")\n                st.write(skill_ref_df[['file', 'model', 'skill', 'title', 'snippet', 'url']].rename(columns={'title': 'Title', 'snippet': 'Snippet', 'url': 'URL'}), unsafe_allow_html=True)\n            else:\n                st.info(\"No skill-driven web references found.\")\n    else:\n        queries = sorted(set(r['query'] for r in refs if 'query' in r))\n        selected_query = st.selectbox(\"Filter by topic/query:\", [\"All\"] + queries)\n        if selected_query == \"All\":\n            filtered_refs = refs\n        else:\n            filtered_refs = [r for r in refs if r.get('query') == selected_query]\n        # --- Search bar for all columns ---\n        search_term = st.text_input(\"Search across all columns:\")\n        if search_term:\n            search_term_lower = search_term.lower()\n            filtered_refs = [r for r in filtered_refs if any(search_term_lower in str(r.get(col, '')).lower() for col in ['title', 'snippet', 'url'])]\n        if filtered_refs:\n            ref_df = pd.DataFrame(filtered_refs)\n            # Make URLs clickable\n            ref_df['url'] = ref_df['url'].apply(lambda x: f\"[link]({x})\" if x else \"\")\n            st.write(ref_df[['title', 'snippet', 'url']].rename(columns={'title': 'Title', 'snippet': 'Snippet', 'url': 'URL'}), unsafe_allow_html=True)\n        else:\n            st.info(\"No references found for this topic or search.\")\n\nelif view_option == \"Generate Report\":\n    st.header(\"Generate Skills Gap Report\")\n    st.write(\"Select the output format and generate a comprehensive report from the ingested and analyzed data.\")\n    report_format = st.radio(\"Select report format:\", [\"Markdown (.md)\", \"Quarto (.qmd)\"])\n\n    # --- Document selection for RAG Q&A and report generation ---\n    all_filenames = [f[\"filename\"] for f in categorized_skills]\n    st.markdown(\"**Select which documents to include in RAG Q&A and report generation:**\")\n    selected_docs = st.multiselect(\"Documents:\", all_filenames, default=all_filenames)\n    st.info(f\"{len(selected_docs)} document(s) selected for RAG Q&A and report generation.\")\n\n    # --- Skill-driven web references selection for report ---\n    skill_web_refs = st.session_state.get(\"skill_web_refs\", {})\n    # Gather all cached references for selected files\n    all_refs = []\n    for cache_key, refs in skill_web_refs.items():\n        # cache_key is a tuple of filenames\n        if any(f in selected_docs for f in cache_key):\n            all_refs.extend(refs)\n    # Remove duplicates (by title+url+skill+file)\n    seen = set()\n    unique_refs = []\n    for r in all_refs:\n        key = (r.get('title',''), r.get('url',''), r.get('skill',''), r.get('file',''))\n        if key not in seen:\n            unique_refs.append(r)\n            seen.add(key)\n    # Let user select which references to include\n    st.markdown(\"**Select which skill-driven web references to include in the report:**\")\n    if unique_refs:\n        ref_options = [f\"{r['file']} | {r['model']} | {r['skill']} | {r['title']}\" for r in unique_refs]\n        selected_ref_labels = st.multiselect(\"References:\", ref_options, default=ref_options)\n        selected_refs = [r for r, label in zip(unique_refs, ref_options) if label in selected_ref_labels]\n    else:\n        st.info(\"No cached skill-driven web references found for selected files.\")\n        selected_refs = []\n\n    # If user doesn't select any, export all by default\n    if not selected_refs:\n        selected_refs = unique_refs\n\n    # --- Sprint 3: Quarto Template Rendering & UI Integration ---\n    st.subheader(\"Render Quarto Template (HTML/PDF)\")\n    TEMPLATE_PATH = os.path.join(BASE_DIR, 'standards', 'skills_gap_template.qmd')\n    TEMPLATE_HTML = os.path.join(DOC_DIR, 'skills_gap_template.html')\n    TEMPLATE_PDF = os.path.join(DOC_DIR, 'skills_gap_template.pdf')\n    TEMPLATE_QMD = os.path.join(DOC_DIR, 'skills_gap_template.qmd')\n    # Copy template to doc dir if not present\n    if not os.path.exists(TEMPLATE_QMD):\n        with open(TEMPLATE_PATH, 'r', encoding='utf-8') as src, open(TEMPLATE_QMD, 'w', encoding='utf-8') as dst:\n            dst.write(src.read())\n\n    col1, col2 = st.columns(2)\n    with col1:\n        st.markdown(\"**Report Source Files**\")\n        if os.path.exists(MD_REPORT):\n            st.info(f\"Markdown Report (.md) saved at: {MD_REPORT}\")\n        if os.path.exists(QMD_REPORT):\n            st.info(f\"Quarto Report (.qmd) saved at: {QMD_REPORT}\")\n        if os.path.exists(TEMPLATE_QMD):\n            st.info(f\"Template Quarto File (.qmd) saved at: {TEMPLATE_QMD}\")\n    with col2:\n        st.markdown(\"**Rendered Output Files**\")\n        if os.path.exists(HTML_REPORT):\n            file_url_info(HTML_REPORT, \"Open HTML Report in Browser\")\n        if os.path.exists(PDF_REPORT):\n            file_url_info(PDF_REPORT, \"Open PDF Report in Browser\")\n        if os.path.exists(TEMPLATE_HTML):\n            file_url_info(TEMPLATE_HTML, \"Open Template HTML in Browser\")\n        if os.path.exists(TEMPLATE_PDF):\n            file_url_info(TEMPLATE_PDF, \"Open Template PDF in Browser\")\n\n    show_links = st.radio(\"Show/Hide links to generated files:\", [\"Show links\", \"Hide links\"], index=0)\n    if show_links == \"Hide links\":\n        st.info(\"Links to generated files are hidden. Use the radio button to show them.\")\n    else:\n        st.success(\"Links to all generated and template files are shown above.\")\n\n    if st.button(\"Render Quarto Template (HTML/PDF)\"):\n        def run_render():\n            try:\n                render_quarto_template(TEMPLATE_PATH, DOC_DIR)\n            except Exception as e:\n                st.error(f\"Template rendering failed: {e}\")\n        threading.Thread(target=run_render, daemon=True).start()\n        st.info(\"Template rendering started in the background. Refresh or wait for links to update.\")\n\n    # --- Existing report generation logic ---\n    st.subheader(\"Generate Data Report (from project analysis)\")\n    if st.button(\"Generate Report\"):\n        with st.spinner(\"Generating report. This may take a minute...\"):\n            # Filter categorized_skills to only selected_docs\n            selected_skills = [f for f in categorized_skills if f[\"filename\"] in selected_docs]\n            # Save filtered skills to a temp file for the report script\n            with tempfile.NamedTemporaryFile(\"w\", delete=False, suffix=\".json\") as tf:\n                json.dump(selected_skills, tf)\n                temp_skills_path = tf.name\n            # Save selected web references to a temp file for the report script\n            with tempfile.NamedTemporaryFile(\"w\", delete=False, suffix=\".json\") as tf_refs:\n                json.dump(selected_refs, tf_refs)\n                temp_refs_path = tf_refs.name\n            # Pass the temp file paths to the report script via env var\n            env = os.environ.copy()\n            env[\"SKILLS_PATH\"] = temp_skills_path\n            env[\"WEBREFS_PATH\"] = temp_refs_path\n            if report_format == \"Markdown (.md)\":\n                result = subprocess.run([\"python3\", os.path.join(TOOLS_DIR, \"generate_document.py\")], cwd=BASE_DIR, capture_output=True, text=True, env=env)\n                st.success(\"Markdown report generated!\")\n                st.info(f\"Markdown Report (.md) saved at: {MD_REPORT}\")\n            else:\n                result = subprocess.run([\"python3\", os.path.join(TOOLS_DIR, \"generate_document_qmd.py\")], cwd=BASE_DIR, capture_output=True, text=True, env=env)\n                st.success(\"Quarto report generated!\")\n                st.info(f\"Quarto Report (.qmd) saved at: {QMD_REPORT}\")\n                file_url_info(HTML_REPORT, \"Open HTML Report in Browser\")\n                file_url_info(PDF_REPORT, \"Open PDF Report in Browser\")\n            st.text_area(\"Script Output\", result.stdout + '\\n' + result.stderr, height=200)\n            os.remove(temp_skills_path)\n            os.remove(temp_refs_path)\n    else:\n        if os.path.exists(MD_REPORT):\n            st.info(f\"Markdown Report (.md) saved at: {MD_REPORT}\")\n        if os.path.exists(QMD_REPORT):\n            st.info(f\"Quarto Report (.qmd) saved at: {QMD_REPORT}\")\n        if os.path.exists(HTML_REPORT):\n            file_url_info(HTML_REPORT, \"Open HTML Report in Browser\")\n        if os.path.exists(PDF_REPORT):\n            file_url_info(PDF_REPORT, \"Open PDF Report in Browser\")\n\n"
  }
]